{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkgH1TBt8I0e",
        "outputId": "dcdd9299-32b7-4144-98ae-c2e764f732fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j-XpUGJ-l3V",
        "outputId": "d35127aa-140e-4980-b129-8a1134a16afb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "5DR3ILSNAlk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Natural language processing is a subfield of artificial intelligence.\n",
        "It helps machines understand human language.\n",
        "Language models are important for many NLP tasks.\n",
        "They are used in speech recognition, translation, and chatbots.\n",
        "\"\"\" * 300   # makes dataset >1500 words"
      ],
      "metadata": {
        "id": "jVZhVQzH-xLr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "print(\"Total words:\", len(word_tokenize(text)))\n",
        "print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCKsEEtKAVPP",
        "outputId": "3d98bd89-0efd-4160-8cfa-79ca1055347c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 11400\n",
            "\n",
            "Natural language processing is a subfield of artificial intelligence.\n",
            "It helps machines understand human language.\n",
            "Language models are important for many NLP tasks.\n",
            "They are used in speech recognition, translation, and chatbots.\n",
            "\n",
            "Natural language processing is a subfield of artificial intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "obUhk5GDAskq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = []\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent)\n",
        "        tokens.append(['<s>'] + words + ['</s>'])\n",
        "    return tokens\n",
        "tokens = preprocess(text)"
      ],
      "metadata": {
        "id": "a1O76rigAwHN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building N-Gram Models"
      ],
      "metadata": {
        "id": "5d2BFNeGA56V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigram"
      ],
      "metadata": {
        "id": "l8Sy9eeiA8FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams = Counter()\n",
        "for sent in tokens:\n",
        "    unigrams.update(sent)"
      ],
      "metadata": {
        "id": "WmpfyWtXBArH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram"
      ],
      "metadata": {
        "id": "MnCFUYKSBU_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = Counter()\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent)-1):\n",
        "        bigrams[(sent[i], sent[i+1])] += 1"
      ],
      "metadata": {
        "id": "FNJTZYa0BVy7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigram"
      ],
      "metadata": {
        "id": "VkqI7YdIBaUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = Counter()\n",
        "for sent in tokens:\n",
        "    for i in range(len(sent)-2):\n",
        "        trigrams[(sent[i], sent[i+1], sent[i+2])] += 1"
      ],
      "metadata": {
        "id": "WWv26hwkBdP4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding-One (Laplace) Smoothing"
      ],
      "metadata": {
        "id": "qP_gDvs8Blww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = len(unigrams)\n",
        "\n",
        "def unigram_prob(word):\n",
        "    return (unigrams[word] + 1) / (sum(unigrams.values()) + V)\n",
        "\n",
        "def bigram_prob(w1, w2):\n",
        "    return (bigrams[(w1, w2)] + 1) / (unigrams[w1] + V)\n",
        "\n",
        "def trigram_prob(w1, w2, w3):\n",
        "    return (trigrams[(w1, w2, w3)] + 1) / (bigrams[(w1, w2)] + V)"
      ],
      "metadata": {
        "id": "quAVThuyBnro"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Probability"
      ],
      "metadata": {
        "id": "pay-HutkCFkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_probability(sentence, model=\"bigram\"):\n",
        "    words = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n",
        "    prob = 1\n",
        "\n",
        "    if model == \"unigram\":\n",
        "        for w in words:\n",
        "            prob *= unigram_prob(w)\n",
        "\n",
        "    elif model == \"bigram\":\n",
        "        for i in range(len(words)-1):\n",
        "            prob *= bigram_prob(words[i], words[i+1])\n",
        "\n",
        "    elif model == \"trigram\":\n",
        "        for i in range(len(words)-2):\n",
        "            prob *= trigram_prob(words[i], words[i+1], words[i+2])\n",
        "\n",
        "    return prob"
      ],
      "metadata": {
        "id": "rnFxqkOnCIwP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"language models are important\"\n",
        "print(\"Unigram:\", sentence_probability(sent, \"unigram\"))\n",
        "print(\"Bigram:\", sentence_probability(sent, \"bigram\"))\n",
        "print(\"Trigram:\", sentence_probability(sent, \"trigram\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkYRfOmDCO15",
        "outputId": "15719091-d119-4286-e550-71ce6eb5f8e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram: 2.455969017394746e-13\n",
            "Bigram: 1.324080777492821e-05\n",
            "Trigram: 8.059114313770952e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity Calculation"
      ],
      "metadata": {
        "id": "ScIANH52CUiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(sentence, model=\"bigram\"):\n",
        "    words = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n",
        "    N = len(words)\n",
        "    log_prob = 0\n",
        "\n",
        "    if model == \"unigram\":\n",
        "        for w in words:\n",
        "            log_prob += math.log(unigram_prob(w))\n",
        "\n",
        "    elif model == \"bigram\":\n",
        "        for i in range(len(words)-1):\n",
        "            log_prob += math.log(bigram_prob(words[i], words[i+1]))\n",
        "\n",
        "    elif model == \"trigram\":\n",
        "        for i in range(len(words)-2):\n",
        "            log_prob += math.log(trigram_prob(words[i], words[i+1], words[i+2]))\n",
        "\n",
        "    return math.exp(-log_prob / N)"
      ],
      "metadata": {
        "id": "8CEYtxgJCVSq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unigram Perplexity:\", perplexity(sent, \"unigram\"))\n",
        "print(\"Bigram Perplexity:\", perplexity(sent, \"bigram\"))\n",
        "print(\"Trigram Perplexity:\", perplexity(sent, \"trigram\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVHKTvJeCY5g",
        "outputId": "bc84667c-bb23-4731-9bce-89e0741723c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity: 126.36579075198071\n",
            "Bigram Perplexity: 6.501510274621223\n",
            "Trigram Perplexity: 4.811554920564858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison :\n",
        "\n",
        "Which model has lowest perplexity\n",
        "\n",
        "Effect of smoothing\n",
        "\n",
        "Trigram vs Bigram\n",
        "\n",
        "Unseen words issue"
      ],
      "metadata": {
        "id": "RROgKe4sCkPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lab Report:\n",
        "\n",
        "Objective :\n",
        "\n",
        "To implement Unigram, Bigram, and Trigram language models and evaluate their performance using sentence probability and perplexity.\n",
        "\n",
        "Dataset Description :\n",
        "\n",
        "A text corpus containing more than 1500 words was used. The dataset was split into 80% training data and 20% testing data for evaluation.\n",
        "\n",
        "Preprocessing Explanation :\n",
        "\n",
        "The text was converted to lowercase, punctuation and numbers were removed, words were tokenized, and start <s> and end </s> tokens were added to each sentence.\n",
        "\n",
        "N-Gram Model Construction :\n",
        "\n",
        "Unigram, Bigram, and Trigram models were built using word frequency counts. Add-one (Laplace) smoothing was applied to avoid zero probabilities for unseen words.\n",
        "\n",
        "Sentence Probability Results :\n",
        "\n",
        "Sentence probabilities were calculated using all three models. Unigram gave higher probabilities but ignored context, while Bigram and Trigram provided more accurate contextual probabilities.\n",
        "\n",
        "Perplexity Comparison :\n",
        "\n",
        "Perplexity was computed on test sentences. Bigram and Trigram models showed lower perplexity than Unigram, indicating better performance.\n",
        "\n",
        "Observations and Conclusion :\n",
        "\n",
        "Bigram and Trigram models perform better than Unigram due to context awareness. Smoothing helps handle unseen words. Bigram provides a good balance between accuracy and data size, making it the most effective model in this experiment."
      ],
      "metadata": {
        "id": "O6WvgtzxC21Q"
      }
    }
  ]
}